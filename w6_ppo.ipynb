{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "mount_file_id": "1JqNKmcR28swGrcq9I0XHufEQKllfnaSW",
      "authorship_tag": "ABX9TyN0xSnks1BXt/1eXa+tCrWY"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RGwLYj46i6Vm"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoModelForSequenceClassification, AutoTokenizer\n",
        "from peft import PeftModel\n",
        "from transformers import BertForSequenceClassification # Import a specific model class if needed\n",
        "\n",
        "# --- 1. LOAD TOKENIZER ---\n",
        "# + Be sure to be consistent in using the Base Model's Tokeniser\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-0.6B\")\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# --- 2. LOADING THE POLICY MODEL---\n",
        "policy_base_model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen3-0.6B\")\n",
        "policy_model = PeftModel.from_pretrained(policy_base_model, \"phh/Qwen3-0.6B-TLDR-Lora\")\n",
        "\n",
        "print(\"âœ… Policy Model Loaded\")\n",
        "\n",
        "\n",
        "# --- 3. LOAD THE REWARD MODEL ---\n",
        "# This model gives you the score.\n",
        "try:\n",
        "    reward_model = AutoModelForSequenceClassification.from_pretrained(\"CarperAI/openai_summarize_tldr_rm_checkpoint\")\n",
        "except ValueError:\n",
        "    reward_model = BertForSequenceClassification.from_pretrained(\"CarperAI/openai_summarize_tldr_rm_checkpoint\")\n",
        "\n",
        "\n",
        "reward_model.eval() # Set to evaluation mode\n",
        "# reward_model.to(\"cuda\")\n",
        "\n",
        "print(\"âœ… Reward Model Loaded\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# A simple prompt to test with\n",
        "prompt_text = \"Human: Write a short, upbeat summary of the movie 'The Matrix'.\\n\\nAssistant:\"\n",
        "\n",
        "# Tokenize the prompt\n",
        "prompt_inputs = tokenizer(prompt_text, return_tensors=\"pt\") #.to(\"cuda\")\n",
        "\n",
        "# Generate a response\n",
        "# response_tensors will include the prompt text\n",
        "response_tensors = policy_model.generate(\n",
        "    input_ids=prompt_inputs.input_ids,\n",
        "    attention_mask=prompt_inputs.attention_mask,\n",
        "    max_new_tokens=60, # Keep it short\n",
        "    pad_token_id=tokenizer.pad_token_id\n",
        ")\n",
        "\n",
        "# Here we decode the generated text to see what you have\n",
        "response_text = tokenizer.decode(response_tensors[0], skip_special_tokens=True)\n",
        "print(\"\\n--- Generated Response ---\")\n",
        "print(response_text)\n",
        "with torch.no_grad(): # Use no_grad if you're only inspecting, not training yet\n",
        "    outputs = policy_model(input_ids=response_tensors)\n",
        "    logits = outputs.logits\n",
        "\n",
        "print(\"\\n--- Forward Pass Results ---\")\n",
        "print(\"âœ… Success! You've got the logits.\")\n",
        "print(f\"Shape of logits tensor: {logits.shape}\")\n",
        "# This shape is (batch_size, sequence_length, vocab_size)"
      ],
      "metadata": {
        "id": "qoeYTVZNn2oh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-0.6B\") # Use the tokenizer from the first cell\n",
        "# reward_model = BertForSequenceClassification.from_pretrained(\"CarperAI/openai_summarize_tldr_rm_checkpoint\")\n",
        "# reward_model.eval()\n",
        "\n",
        "prompt_text = \"Human: Write a short, upbeat summary of the movie 'The Matrix'.\\n\\nAssistant:\"\n",
        "generated_summary = \" A computer hacker learns from mysterious rebels about the true nature of his reality and his role in the war against its controllers.\"\n",
        "\n",
        "# Combine them into the format the reward model expects\n",
        "text_to_score = prompt_text + generated_summary\n",
        "\n",
        "print(\"--- Text being scored ---\")\n",
        "print(text_to_score)\n",
        "\n",
        "\n",
        "\n",
        "# 1. Tokenize the full text using the tokenizer loaded in the first cell\n",
        "reward_inputs = tokenizer(\n",
        "    text_to_score,\n",
        "    return_tensors=\"pt\",\n",
        "    padding=True,\n",
        "    truncation=True\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "g0FV7Bx-jBQA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reward_inputs = tokenizer(\n",
        "    response_text,\n",
        "    return_tensors=\"pt\",\n",
        "    padding=True,\n",
        "    truncation=True\n",
        ") #.to(\"cuda\")\n",
        "\n",
        "# Get the score!\n",
        "with torch.no_grad():\n",
        "    reward_score = reward_model(**reward_inputs).logits[0]\n",
        "\n",
        "print(\"\\n--- The Win for Today ---\")\n",
        "print(f\"ðŸ† Reward Score: {reward_score.item()}\")"
      ],
      "metadata": {
        "id": "Munwo9aRjGU-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        ")\n",
        "from typing import Dict, Any\n",
        "from dataclasses import dataclass # Import dataclass\n",
        "from transformers.tokenization_utils_base import PreTrainedTokenizerBase # Import PreTrainedTokenizerBase\n",
        "from typing import List, Union # Import List and Union\n",
        "\n",
        "# --- 1. Configuration ---\n",
        "# Model and dataset paths\n",
        "BASE_MODEL_NAME = \"Qwen/Qwen3-0.6B\"\n",
        "DATASET_PATH = \"/content/drive/MyDrive/MLI/mliw6/data/asian_dad_responses_2000.parquet\"\n",
        "OUTPUT_MODEL_PATH = \"/content/drive/MyDrive/MLI/mliw6\"\n",
        "\n",
        "# Training parameters\n",
        "LEARNING_RATE = 2e-5\n",
        "NUM_EPOCHS = 1\n",
        "BATCH_SIZE = 4 # Adjust based on your GPU memory\n",
        "MAX_LENGTH = 512 # Max sequence length for tokenization\n",
        "\n",
        "# --- 2. Load Tokenizer and Model ---\n",
        "print(\"Loading tokenizer and model...\")\n",
        "\n",
        "# We use a tokeniser to convert text into a format the model understands.\n",
        "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_NAME, trust_remote_code=True)\n",
        "\n",
        "# In this case, the reward model essentially serves a classifier that outputs a single score.\n",
        "# We use AutoModelForSequenceClassification with num_labels=1 to achieve this.\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    BASE_MODEL_NAME,\n",
        "    num_labels=1, # We want a single scalar output (the reward score)\n",
        "    trust_remote_code=True,\n",
        "    torch_dtype=torch.bfloat16 # Use bfloat16 for memory efficiency\n",
        ")\n",
        "\n",
        "# Decoder-only models like Qwen often don't have a pad token.\n",
        "# We set it to the End-Of-Sentence token for padding purposes.\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    model.config.pad_token_id = model.config.eos_token_id\n",
        "\n",
        "# Explicitly set the model's padding token ID if it's not set\n",
        "if model.config.pad_token_id is None:\n",
        "    model.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "\n",
        "print(\"Tokenizer and model loaded successfully.\")\n",
        "\n",
        "\n",
        "# --- 3. Load and Prepare the Dataset ---\n",
        "print(f\"Loading dataset from '{DATASET_PATH}'...\")\n",
        "df = pd.read_parquet(DATASET_PATH)\n",
        "dataset = Dataset.from_pandas(df)\n",
        "\n",
        "def preprocess_function(examples: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    This function tokenizes the chosen and rejected responses for each prompt.\n",
        "    It creates two separate inputs for the reward model to score.\n",
        "    \"\"\"\n",
        "    new_examples = {\n",
        "        \"input_ids_chosen\": [], \"attention_mask_chosen\": [],\n",
        "        \"input_ids_rejected\": [], \"attention_mask_rejected\": [],\n",
        "    }\n",
        "    for prompt, chosen, rejected in zip(examples[\"prompt\"], examples[\"chosen\"], examples[\"rejected\"]):\n",
        "        # Format the text as a conversation turn\n",
        "        text_chosen = f\"Human: {prompt}\\n\\nAssistant: {chosen}\"\n",
        "        text_rejected = f\"Human: {prompt}\\n\\nAssistant: {rejected}\"\n",
        "\n",
        "        # Tokenize both versions\n",
        "        tokenized_chosen = tokenizer(text_chosen, max_length=MAX_LENGTH, truncation=True)\n",
        "        tokenized_rejected = tokenizer(text_rejected, max_length=MAX_LENGTH, truncation=True)\n",
        "\n",
        "        new_examples[\"input_ids_chosen\"].append(tokenized_chosen[\"input_ids\"])\n",
        "        new_examples[\"attention_mask_chosen\"].append(tokenized_chosen[\"attention_mask\"])\n",
        "        new_examples[\"input_ids_rejected\"].append(tokenized_rejected[\"input_ids\"])\n",
        "        new_examples[\"attention_mask_rejected\"].append(tokenized_rejected[\"attention_mask\"])\n",
        "\n",
        "    return new_examples\n",
        "\n",
        "# Apply the preprocessing to the entire dataset\n",
        "print(\"Preprocessing dataset...\")\n",
        "tokenized_dataset = dataset.map(preprocess_function, batched=True, num_proc=4, remove_columns=dataset.column_names)\n",
        "print(\"Dataset preprocessing complete.\")\n",
        "\n",
        "\n",
        "# --- 4. Custom Reward Trainer ---\n",
        "# We create a custom Trainer to implement the ranking loss function.\n",
        "class RewardTrainer(Trainer):\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None): # <--- ADDED num_items_in_batch=None\n",
        "        \"\"\"\n",
        "        Computes the loss for reward model training. The goal is to maximize the\n",
        "        margin between the scores of the chosen and rejected responses.\n",
        "        \"\"\"\n",
        "        # Get the scores for the chosen responses\n",
        "        rewards_chosen = model(\n",
        "            input_ids=inputs[\"input_ids_chosen\"],\n",
        "            attention_mask=inputs[\"attention_mask_chosen\"]\n",
        "        )[0]\n",
        "\n",
        "        # Get the scores for the rejected responses\n",
        "        rewards_rejected = model(\n",
        "            input_ids=inputs[\"input_ids_rejected\"],\n",
        "            attention_mask=inputs[\"attention_mask_rejected\"]\n",
        "        )[0]\n",
        "\n",
        "        # The loss is calculated to ensure score(chosen) > score(rejected).\n",
        "        # We use the log-sigmoid function for numerical stability.\n",
        "        loss = -torch.nn.functional.logsigmoid(rewards_chosen - rewards_rejected).mean()\n",
        "\n",
        "        if return_outputs:\n",
        "            return loss, {\"rewards_chosen\": rewards_chosen, \"rewards_rejected\": rewards_rejected}\n",
        "        return loss\n",
        "\n",
        "# --- Custom Data Collator ---\n",
        "@dataclass\n",
        "class RewardModelDataCollator:\n",
        "    \"\"\"\n",
        "    Data collator for reward model training.\n",
        "    Pads the chosen and rejected responses separately.\n",
        "    \"\"\"\n",
        "    tokenizer: PreTrainedTokenizerBase\n",
        "    padding: Union[bool, str] = True\n",
        "    max_length: int = None\n",
        "    pad_to_multiple_of: int = None\n",
        "\n",
        "    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
        "        # Separate chosen and rejected features\n",
        "        chosen_features = []\n",
        "        rejected_features = []\n",
        "        for feature in features:\n",
        "            chosen_features.append({\n",
        "                \"input_ids\": feature[\"input_ids_chosen\"],\n",
        "                \"attention_mask\": feature[\"attention_mask_chosen\"],\n",
        "            })\n",
        "            rejected_features.append({\n",
        "                \"input_ids\": feature[\"input_ids_rejected\"],\n",
        "                \"attention_mask\": feature[\"attention_mask_rejected\"],\n",
        "            })\n",
        "\n",
        "        # Pad chosen and rejected features separately\n",
        "        padded_chosen_features = self.tokenizer.pad(\n",
        "            chosen_features,\n",
        "            padding=self.padding,\n",
        "            max_length=self.max_length,\n",
        "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "        padded_rejected_features = self.tokenizer.pad(\n",
        "            rejected_features,\n",
        "            padding=self.padding,\n",
        "            max_length=self.max_length,\n",
        "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "\n",
        "        # Combine back into a single dictionary with original keys\n",
        "        batch = {\n",
        "            \"input_ids_chosen\": padded_chosen_features[\"input_ids\"],\n",
        "            \"attention_mask_chosen\": padded_chosen_features[\"attention_mask\"],\n",
        "            \"input_ids_rejected\": padded_rejected_features[\"input_ids\"],\n",
        "            \"attention_mask_rejected\": padded_rejected_features[\"attention_mask\"], # Corrected key\n",
        "        }\n",
        "        return batch\n",
        "\n",
        "\n",
        "# --- 5. Training ---\n",
        "print(\"Setting up training arguments...\")\n",
        "\n",
        "# Define the arguments for the training process\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=OUTPUT_MODEL_PATH,\n",
        "    num_train_epochs=NUM_EPOCHS,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    logging_steps=10,\n",
        "    save_strategy=\"epoch\",\n",
        "    report_to=\"none\", # Disable wandb/tensorboard reporting for simplicity\n",
        "    bf16=True, # Enable bfloat16 mixed-precision training\n",
        "    remove_unused_columns=False, # Keep columns not used by the model's forward pass\n",
        ")\n",
        "\n",
        "# Instantiate our custom trainer, providing the custom data collator\n",
        "trainer = RewardTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    tokenizer=tokenizer, # Keep the tokenizer here for the Trainer's internal use if needed,\n",
        "    data_collator=RewardModelDataCollator(tokenizer=tokenizer, max_length=MAX_LENGTH) # <--- Use the custom collator\n",
        ")\n",
        "\n",
        "print(\"--- Starting Reward Model Training ---\")\n",
        "trainer.train()\n",
        "print(\"--- Training Complete ---\")\n",
        "\n",
        "\n",
        "# --- 6. Save the Final Model ---\n",
        "print(f\"Saving the trained reward model to '{OUTPUT_MODEL_PATH}'...\")\n",
        "trainer.save_model(OUTPUT_MODEL_PATH)\n",
        "tokenizer.save_pretrained(OUTPUT_MODEL_PATH)\n",
        "print(\"Model saved successfully. You can now use this model for PPO.\")"
      ],
      "metadata": {
        "id": "WvMZBkXN5O-A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install trl\n"
      ],
      "metadata": {
        "id": "814C7xsk8dwl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers AdamW\n"
      ],
      "metadata": {
        "id": "21GydIOx97I6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModelForCausalLM\n",
        "from peft import PeftModel\n",
        "from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead\n",
        "from tqdm import tqdm\n",
        "\n",
        "# --- 1. Configuration ---\n",
        "# Model and dataset paths\n",
        "POLICY_BASE_MODEL = \"Qwen/Qwen3-0.6B\"\n",
        "POLICY_ADAPTER_MODEL = \"phh/Qwen3-0.6B-TLDR-Lora\"\n",
        "# This is the path to the directory containing your trained reward model's config and weights\n",
        "REWARD_MODEL_PATH = \"/content/drive/MyDrive/MLI/mliw6/checkpoint-500\"\n",
        "DATASET_PATH = \"/content/drive/MyDrive/MLI/mliw6/data/asian_dad_responses_2000.parquet\"\n",
        "FINAL_MODEL_OUTPUT_PATH = \"./policy_model_asian_dad_ppo\"\n",
        "\n",
        "# PPO Training parameters\n",
        "LEARNING_RATE = 1.41e-5\n",
        "BATCH_SIZE = 8 # Number of prompts to process at once\n",
        "MINI_BATCH_SIZE = 2 # Batch size for policy updates\n",
        "EPOCHS = 1 # Number of times to iterate over the dataset\n",
        "KL_PENALTY = 0.2 # KL penalty coefficient (controls how much the policy can change)\n",
        "GENERATION_KWARGS = { # Parameters for the .generate() method\n",
        "    \"max_new_tokens\": 80,\n",
        "    \"top_k\": 0,\n",
        "    \"top_p\": 1.0,\n",
        "    \"do_sample\": True,\n",
        "    \"pad_token_id\": None, # Will be set later\n",
        "}\n",
        "\n",
        "# --- 2. Load Models and Tokenizer ---\n",
        "print(\"--- Loading Models and Tokenizer ---\")\n",
        "\n",
        "# The tokenizer must be the same for all models\n",
        "tokenizer = AutoTokenizer.from_pretrained(POLICY_BASE_MODEL, trust_remote_code=True)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "GENERATION_KWARGS[\"pad_token_id\"] = tokenizer.pad_token_id\n",
        "\n",
        "# Load the policy model (the \"writer\" we want to train)\n",
        "# We use AutoModelForCausalLMWithValueHead from TRL, which adds a value head\n",
        "# for the PPO algorithm. This head helps estimate the \"value\" of a state.\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    POLICY_BASE_MODEL,\n",
        "    trust_remote_code=True,\n",
        "    torch_dtype=torch.bfloat16,\n",
        ")\n",
        "policy_model = PeftModel.from_pretrained(base_model, POLICY_ADAPTER_MODEL)\n",
        "model = AutoModelForCausalLMWithValueHead.from_pretrained(policy_model)\n",
        "print(\"âœ… Policy Model with Value Head Loaded\")\n",
        "\n",
        "# Load the reference model (a frozen copy of the initial policy)\n",
        "# This is used to calculate the KL penalty.\n",
        "ref_model = AutoModelForCausalLMWithValueHead.from_pretrained(policy_model)\n",
        "print(\"âœ… Reference Model Loaded\")\n",
        "\n",
        "# Load the reward model (the \"judge\")\n",
        "# This is the model you just trained.\n",
        "reward_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    REWARD_MODEL_PATH,\n",
        "    num_labels=1,\n",
        "    torch_dtype=torch.bfloat16,\n",
        ")\n",
        "# The reward model's tokenizer is not needed as we use the main one.\n",
        "print(\"âœ… Reward Model Loaded\")\n",
        "\n",
        "# Move models to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "ref_model.to(device)\n",
        "reward_model.to(device)\n",
        "print(f\"Models moved to device: {device}\")\n",
        "\n",
        "\n",
        "# --- 3. Prepare Dataset and PPO Trainer ---\n",
        "print(\"\\n--- Preparing Dataset and PPO Trainer ---\")\n",
        "\n",
        "# We only need the 'prompt' column for PPO training.\n",
        "df = pd.read_parquet(DATASET_PATH)\n",
        "dataset = Dataset.from_pandas(df[[\"prompt\"]])\n",
        "\n",
        "def tokenize_prompt(example):\n",
        "    \"\"\"Tokenizes the prompt text.\"\"\"\n",
        "    # We format the prompt to guide the model's generation.\n",
        "    formatted_prompt = f\"Human: {example['prompt']}\\n\\nAssistant:\"\n",
        "    tokenized = tokenizer(formatted_prompt, truncation=True, max_length=512)\n",
        "    tokenized[\"query\"] = tokenizer.decode(tokenized[\"input_ids\"])\n",
        "    return tokenized\n",
        "\n",
        "dataset = dataset.map(tokenize_prompt)\n",
        "dataset.set_format(type=\"torch\")\n",
        "\n",
        "# Instantiate the PPOTrainer\n",
        "# We pass the configuration arguments directly to the trainer and handle the\n",
        "# tokenizer separately to avoid versioning errors.\n",
        "ppo_trainer = PPOTrainer(\n",
        "    model=model,\n",
        "    ref_model=ref_model,\n",
        "    dataset=dataset,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    mini_batch_size=MINI_BATCH_SIZE,\n",
        "    kl_penalty=KL_PENALTY,\n",
        ")\n",
        "# Manually set the tokenizer on the trainer object. This is a robust way\n",
        "# to handle different TRL versions, as some don't accept it in the constructor.\n",
        "ppo_trainer.tokenizer = tokenizer\n",
        "\n",
        "print(\"âœ… PPO Trainer Initialized\")\n",
        "\n",
        "\n",
        "# --- 4. The PPO Training Loop ---\n",
        "print(\"\\n--- Starting PPO Training ---\")\n",
        "progress_bar = tqdm(range(ppo_trainer.dataloader.num_batches * EPOCHS), desc=\"PPO Training\")\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    for batch in ppo_trainer.dataloader:\n",
        "        query_tensors = batch[\"input_ids\"]\n",
        "\n",
        "        # 1. ROLLOUT: Generate responses from the policy model\n",
        "        response_tensors = ppo_trainer.generate(\n",
        "            query_tensors,\n",
        "            return_prompt=False, # We only want the generated part\n",
        "            **GENERATION_KWARGS,\n",
        "        )\n",
        "        batch[\"response\"] = ppo_trainer.tokenizer.batch_decode(response_tensors)\n",
        "\n",
        "        # 2. EVALUATION: Get reward scores from the judge\n",
        "        # We combine the prompt (query) and the generated response to score them.\n",
        "        texts_to_score = [q + r for q, r in zip(batch[\"query\"], batch[\"response\"])]\n",
        "        reward_inputs = ppo_trainer.tokenizer(texts_to_score, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # The reward is a single scalar score for each response.\n",
        "            reward_scores = reward_model(**reward_inputs).logits\n",
        "\n",
        "        rewards = [score for score in reward_scores]\n",
        "\n",
        "        # 3. OPTIMIZATION: Perform a PPO step\n",
        "        # This is where TRL's magic happens. The trainer handles the complex\n",
        "        # calculations for advantages, ratios, and the clipped surrogate objective.\n",
        "        stats = ppo_trainer.step(query_tensors, response_tensors, rewards)\n",
        "\n",
        "        # Log the stats\n",
        "        ppo_trainer.log_stats(stats, batch, rewards)\n",
        "        progress_bar.update(1)\n",
        "\n",
        "progress_bar.close()\n",
        "print(\"--- PPO Training Complete ---\")\n",
        "\n",
        "\n",
        "# --- 5. Save the Final Model ---\n",
        "print(f\"\\nSaving the final PPO-tuned model to '{FINAL_MODEL_OUTPUT_PATH}'...\")\n",
        "ppo_trainer.save_model(FINAL_MODEL_OUTPUT_PATH)\n",
        "# Also save the tokenizer for easy loading later\n",
        "ppo_trainer.tokenizer.save_pretrained(FINAL_MODEL_OUTPUT_PATH)\n",
        "print(\"âœ… Final model saved successfully!\")\n"
      ],
      "metadata": {
        "id": "N6b5f3W58a7X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show trl\n"
      ],
      "metadata": {
        "id": "ucHObOojFN4g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install trl==0.19.1"
      ],
      "metadata": {
        "id": "J1FL0C6PFsqa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModelForCausalLM\n",
        "from peft import PeftModel\n",
        "from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead\n",
        "from tqdm import tqdm\n",
        "\n",
        "# --- 1. Configuration ---\n",
        "# Model and dataset paths\n",
        "POLICY_BASE_MODEL = \"Qwen/Qwen3-0.6B\"\n",
        "POLICY_ADAPTER_MODEL = \"phh/Qwen3-0.6B-TLDR-Lora\"\n",
        "# This is the path to the directory containing your trained reward model's config and weights\n",
        "REWARD_MODEL_PATH = \"/content/drive/MyDrive/MLI/mliw6/checkpoint-500\"\n",
        "DATASET_PATH = \"/content/drive/MyDrive/MLI/mliw6/data/asian_dad_responses_2000.parquet\"\n",
        "FINAL_MODEL_OUTPUT_PATH = \"./policy_model_asian_dad_ppo\"\n",
        "\n",
        "# PPO Training parameters\n",
        "LEARNING_RATE = 1.41e-5\n",
        "BATCH_SIZE = 8 # Number of prompts to process at once\n",
        "MINI_BATCH_SIZE = 2 # Batch size for policy updates\n",
        "EPOCHS = 1 # Number of times to iterate over the dataset\n",
        "KL_PENALTY = 0.2 # KL penalty coefficient (controls how much the policy can change)\n",
        "GENERATION_KWARGS = { # Parameters for the .generate() method\n",
        "    \"max_new_tokens\": 80,\n",
        "    \"top_k\": 0,\n",
        "    \"top_p\": 1.0,\n",
        "    \"do_sample\": True,\n",
        "    \"pad_token_id\": None, # Will be set later\n",
        "}\n",
        "\n",
        "# --- 2. Load Models and Tokenizer ---\n",
        "print(\"--- Loading Models and Tokenizer ---\")\n",
        "\n",
        "# The tokenizer must be the same for all models\n",
        "tokenizer = AutoTokenizer.from_pretrained(POLICY_BASE_MODEL, trust_remote_code=True)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "GENERATION_KWARGS[\"pad_token_id\"] = tokenizer.pad_token_id\n",
        "\n",
        "# Load the policy model (the \"writer\" we want to train)\n",
        "# We use AutoModelForCausalLMWithValueHead from TRL, which adds a value head\n",
        "# for the PPO algorithm. This head helps estimate the \"value\" of a state.\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    POLICY_BASE_MODEL,\n",
        "    trust_remote_code=True,\n",
        "    torch_dtype=torch.bfloat16,\n",
        ")\n",
        "policy_model = PeftModel.from_pretrained(base_model, POLICY_ADAPTER_MODEL)\n",
        "model = AutoModelForCausalLMWithValueHead.from_pretrained(policy_model)\n",
        "print(\"âœ… Policy Model with Value Head Loaded\")\n",
        "\n",
        "# Load the reference model (a frozen copy of the initial policy)\n",
        "# This is used to calculate the KL penalty.\n",
        "ref_model = AutoModelForCausalLMWithValueHead.from_pretrained(policy_model)\n",
        "print(\"âœ… Reference Model Loaded\")\n",
        "\n",
        "# Load the reward model (the \"judge\")\n",
        "# This is the model you just trained.\n",
        "reward_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    REWARD_MODEL_PATH,\n",
        "    num_labels=1,\n",
        "    torch_dtype=torch.bfloat16,\n",
        ")\n",
        "# The reward model's tokenizer is not needed as we use the main one.\n",
        "print(\"âœ… Reward Model Loaded\")\n",
        "\n",
        "# Move models to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "ref_model.to(device)\n",
        "reward_model.to(device)\n",
        "print(f\"Models moved to device: {device}\")\n",
        "\n",
        "\n",
        "# --- 3. Prepare Dataset and PPO Trainer ---\n",
        "print(\"\\n--- Preparing Dataset and PPO Trainer ---\")\n",
        "\n",
        "# We only need the 'prompt' column for PPO training.\n",
        "df = pd.read_parquet(DATASET_PATH)\n",
        "dataset = Dataset.from_pandas(df[[\"prompt\"]])\n",
        "\n",
        "def tokenize_prompt(example):\n",
        "    \"\"\"Tokenizes the prompt text.\"\"\"\n",
        "    # We format the prompt to guide the model's generation.\n",
        "    formatted_prompt = f\"Human: {example['prompt']}\\n\\nAssistant:\"\n",
        "    tokenized = tokenizer(formatted_prompt, truncation=True, max_length=512)\n",
        "    tokenized[\"query\"] = tokenizer.decode(tokenized[\"input_ids\"])\n",
        "    return tokenized\n",
        "\n",
        "dataset = dataset.map(tokenize_prompt)\n",
        "dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"query\"])\n",
        "\n",
        "def collator(data):\n",
        "    return dict((key, [d[key] for d in data]) for key in data[0])\n",
        "\n",
        "# PPOConfig is the modern way to configure the trainer in recent TRL versions.\n",
        "# This object bundles all training parameters cleanly.\n",
        "config = PPOConfig(\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    mini_batch_size=MINI_BATCH_SIZE,\n",
        "    kl_penalty=\"kl\",\n",
        "    adap_kl_ctrl=False, # Set to False for a fixed KL penalty\n",
        "    init_kl_coef=KL_PENALTY,\n",
        "    log_with=\"none\", # Disable wandb/tensorboard reporting\n",
        ")\n",
        "\n",
        "# Instantiate the PPOTrainer using the config object.\n",
        "# This is the standard approach for TRL v0.7.0 and later.\n",
        "ppo_trainer = PPOTrainer(\n",
        "    config=config,\n",
        "    model=model,\n",
        "    ref_model=ref_model,\n",
        "    tokenizer=tokenizer,\n",
        "    dataset=dataset,\n",
        "    data_collator=collator,\n",
        ")\n",
        "print(\"âœ… PPO Trainer Initialized\")\n",
        "\n",
        "\n",
        "# --- 4. The PPO Training Loop ---\n",
        "print(\"\\n--- Starting PPO Training ---\")\n",
        "# The modern PPOTrainer creates its own dataloader from the provided dataset.\n",
        "progress_bar = tqdm(range(len(ppo_trainer.dataloader) * EPOCHS), desc=\"PPO Training\")\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    for batch in ppo_trainer.dataloader:\n",
        "        query_tensors = batch[\"input_ids\"]\n",
        "\n",
        "        # 1. ROLLOUT: Generate responses from the policy model\n",
        "        response_tensors = ppo_trainer.generate(\n",
        "            query_tensors,\n",
        "            return_prompt=False, # We only want the generated part\n",
        "            **GENERATION_KWARGS,\n",
        "        )\n",
        "        batch[\"response\"] = ppo_trainer.tokenizer.batch_decode(response_tensors)\n",
        "\n",
        "        # 2. EVALUATION: Get reward scores from the judge\n",
        "        # We combine the prompt (query) and the generated response to score them.\n",
        "        texts_to_score = [q + r for q, r in zip(batch[\"query\"], batch[\"response\"])]\n",
        "        reward_inputs = ppo_trainer.tokenizer(texts_to_score, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # The reward is a single scalar score for each response.\n",
        "            reward_scores = reward_model(**reward_inputs).logits\n",
        "\n",
        "        rewards = [score for score in reward_scores]\n",
        "\n",
        "        # 3. OPTIMIZATION: Perform a PPO step\n",
        "        # This is where TRL's magic happens. The trainer handles the complex\n",
        "        # calculations for advantages, ratios, and the clipped surrogate objective.\n",
        "        stats = ppo_trainer.step(query_tensors, response_tensors, rewards)\n",
        "\n",
        "        # Log the stats\n",
        "        ppo_trainer.log_stats(stats, batch, rewards)\n",
        "        progress_bar.update(1)\n",
        "\n",
        "progress_bar.close()\n",
        "print(\"--- PPO Training Complete ---\")\n",
        "\n",
        "\n",
        "# --- 5. Save the Final Model ---\n",
        "print(f\"\\nSaving the final PPO-tuned model to '{FINAL_MODEL_OUTPUT_PATH}'...\")\n",
        "ppo_trainer.save_model(FINAL_MODEL_OUTPUT_PATH)\n",
        "# Also save the tokenizer for easy loading later\n",
        "ppo_trainer.tokenizer.save_pretrained(FINAL_MODEL_OUTPUT_PATH)\n",
        "print(\"âœ… Final model saved successfully!\")\n"
      ],
      "metadata": {
        "id": "bn7y_CnvEXst"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
