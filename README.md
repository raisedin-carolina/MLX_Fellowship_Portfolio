# MLX_Fellowship_Portfolio
MLX AI Fellowship Portfolio (June-July 2025)
This repository documents my work from the Machine Learning Institute (MLX) AI Fellowship, an intensive, 6-week MSc-level program (3% acceptance rate) undertaken on a full scholarship.

## The Narrative: Rapid Ascent in AI
I entered this fellowship immediately following my undergraduate exams, possessing a strong foundation in social sciences but no prior experience in coding or machine learning. The program was designed as a high-intensity immersion into both the theoretical foundations and practical implementation of cutting-edge AI.

Over six weeks, I operated within a high-performance research environment, collaborating with and learning from senior AI researchers representing leading institutions and companies such as Oxford, Microsoft, Stanford, and Google. The fellowship's rigorous weekly cycle involved:

Digest: Deep study of seminal academic papers (e.g., "Attention Is All You Need," Word2Vec, PPO).

Present: Articulating and teaching the core mathematical theory back to an elite cohort.


Implement: Building the models from scratch using PyTorch, translating complex theory into functional code under tight deadlines.

This portfolio serves as a record of that accelerated learning process. The code demonstrates the rapid progression from foundational concepts to implementing sophisticated models like a PPO agent for RLHF within a demanding, time-constrained environment. It reflects my ability to quickly acquire and apply complex technical knowledge.

Key Competencies Demonstrated
Accelerated Learning & Adaptation: Proven capacity to master advanced technical concepts and coding practices starting from zero prior experience in an exceptionally short timeframe.


Theory-to-Implementation: Fluidity in translating dense academic research into practical, high-performance PyTorch code.


First-Principles Implementation: Experience building complex architectures like Transformers and PPO from the ground up, ensuring a deep conceptual understanding.


High-Performance Collaboration: Ability to thrive and contribute effectively within a demanding, collaborative environment alongside leading academics and industry professionals.

Grit & Resilience: Successfully navigated a high-pressure, "sink-or-swim" program requiring rapid skill acquisition and consistent delivery.

## Project Showcase
This repository is organized by project. Each folder contains the relevant notebooks and a specific README detailing the objective, technical concepts, and reflections on the learning process during that week.


Focus: Implementing Word2Vec (Skip-gram) from first principles. Understanding context-based embeddings, negative sampling, and PyTorch basics.



Focus: Building the core multi-head self-attention mechanism and components of the Transformer architecture ("Attention Is All You Need") from scratch.



Focus: Implementing a Proximal Policy Optimization (PPO) agent within a Reinforcement Learning from Human Feedback (RLHF) framework, integrating Lora-adapted models. Understanding the modern LLM alignment pipeline.



Focus: Developing custom data loaders, collation scripts, and basic data generation pipelines (e.g., for MNIST variations) to support model training.
